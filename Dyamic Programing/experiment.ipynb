{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils import policy_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 5\n",
    "n_cols = 5\n",
    "n_states = n_rows*n_cols\n",
    "n_actions = 4\n",
    "n_rewards = 1\n",
    "\n",
    "actions = {\n",
    "    \"up\" : 0,\n",
    "    \"right\" : 1,\n",
    "    \"down\" : 2,\n",
    "    \"left\": 3,\n",
    "}\n",
    "\n",
    "coord_A = (1, 0)\n",
    "coord_B = (3, 0)\n",
    "coord_new_A = (1, 4)\n",
    "coord_new_B = (3, 2)\n",
    "reward_A = 10\n",
    "reward_B = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros((n_states, n_actions))\n",
    "probabilities = np.zeros((n_states, n_actions, n_states, n_rewards ))\n",
    "rewards  = np.zeros((n_states, n_actions, n_states, n_rewards ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(n_states):\n",
    "    for a in range(n_actions):\n",
    "        policy[s, a] = 1/n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(n_states):\n",
    "    coord_x = s%n_cols\n",
    "    coord_y = s//n_cols\n",
    "    if (coord_x == coord_A[0] and coord_y == coord_A[1]):\n",
    "        new_state = coord_new_A[0] + coord_new_A[1]*n_cols\n",
    "        probabilities[s, actions[\"up\"], new_state, 0] = 1\n",
    "        probabilities[s, actions[\"down\"], new_state, 0] = 1\n",
    "        probabilities[s, actions[\"right\"], new_state, 0] = 1\n",
    "        probabilities[s, actions[\"left\"], new_state, 0] = 1\n",
    "        rewards[s, actions[\"up\"], new_state, 0] = reward_A\n",
    "        rewards[s, actions[\"down\"], new_state, 0] = reward_A\n",
    "        rewards[s, actions[\"right\"], new_state, 0] = reward_A\n",
    "        rewards[s, actions[\"left\"], new_state, 0] = reward_A    \n",
    "        continue\n",
    "    if (coord_x == coord_B[0] and coord_y == coord_B[1]):\n",
    "        new_state = coord_new_B[0] + coord_new_B[1]*n_cols\n",
    "        probabilities[s, actions[\"up\"], new_state, 0] = 1\n",
    "        probabilities[s, actions[\"down\"], new_state, 0] = 1\n",
    "        probabilities[s, actions[\"right\"], new_state, 0] = 1\n",
    "        probabilities[s, actions[\"left\"], new_state, 0] = 1\n",
    "        rewards[s, actions[\"up\"], new_state, 0] = reward_B\n",
    "        rewards[s, actions[\"down\"], new_state, 0] = reward_B\n",
    "        rewards[s, actions[\"right\"], new_state, 0] = reward_B\n",
    "        rewards[s, actions[\"left\"], new_state, 0] = reward_B    \n",
    "        continue\n",
    "    \n",
    "    \n",
    "    if coord_y == 0:\n",
    "        probabilities[s, actions[\"up\"], s, 0] = 1\n",
    "        rewards[s, actions[\"up\"], s, 0] = -1\n",
    "    else:\n",
    "        probabilities[s, actions[\"up\"], s - n_cols, 0] = 1\n",
    "    if coord_y == n_rows - 1:\n",
    "        probabilities[s, actions[\"down\"], s, 0] = 1\n",
    "        rewards[s, actions[\"down\"], s, 0] = -1\n",
    "    else:\n",
    "        probabilities[s, actions[\"down\"], s + n_cols, 0] = 1\n",
    "    if coord_x == 0:\n",
    "        probabilities[s, actions[\"left\"], s, 0] = 1\n",
    "        rewards[s, actions[\"left\"], s, 0] = -1\n",
    "    else:\n",
    "        probabilities[s, actions[\"left\"], s - 1, 0] = 1\n",
    "    if coord_x == n_cols - 1:\n",
    "        probabilities[s, actions[\"right\"], s, 0] = 1\n",
    "        rewards[s, actions[\"right\"], s, 0] = -1\n",
    "    else:\n",
    "        probabilities[s, actions[\"right\"], s + 1, 0] = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009738395157221826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.36145071,  8.83483335,  4.59290907,  5.71944403,  1.72063588],\n",
       "       [ 1.57547209,  3.05402717,  2.35783398,  2.07631236,  0.69167218],\n",
       "       [ 0.10225986,  0.79318907,  0.7444683 ,  0.44801381, -0.31437833],\n",
       "       [-0.9253625 , -0.38760998, -0.30183935, -0.52646072, -1.12257479],\n",
       "       [-1.81140249, -1.30103866, -1.1836576 , -1.37489437, -1.92608434]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = policy_evaluation(policy, probabilities, rewards, gamma=0.9)\n",
    "result.reshape((n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7730ca559ff8feee379a2381e28d3e6f0cb3410e772809d377ff1828e961366a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
