{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils import policy_evaluation, policy_iteration, value_iteration\n",
    "from environment import GridWorld, generate_grid_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 5\n",
    "n_cols = 5\n",
    "n_states = n_rows*n_cols\n",
    "n_actions = 4\n",
    "n_rewards = 1\n",
    "\n",
    "actions = {\n",
    "    \"up\" : 0,\n",
    "    \"right\" : 1,\n",
    "    \"down\" : 2,\n",
    "    \"left\": 3,\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_A, coord_new_A,  reward_A = (1, 0), (1, 4), 10\n",
    "coord_B, coord_new_B,  reward_B = (3, 0), (3, 2), 5\n",
    "grid_world_1_param = {\n",
    "    \"n_rows\": 5,\n",
    "    \"n_cols\": 5,\n",
    "    \"step_reward\": 0,\n",
    "    \"off_grid_reward\" : -1,\n",
    "    \"special_transitions\": {\n",
    "        coord_A: (coord_new_A, reward_A),\n",
    "        coord_B: (coord_new_B, reward_B),\n",
    "    }\n",
    "}\n",
    "\n",
    "grid_world_1 = GridWorld(**grid_world_1_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.zeros((n_states, n_actions), dtype=np.float64)\n",
    "probabilities, rewards  = grid_world_1.get_dynamics()\n",
    "for s in range(n_states):\n",
    "    for a in range(n_actions):\n",
    "        policy[s, a] = 1/n_actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.31283735,  8.79232751,  4.43054904,  5.32502981,  1.49497109],\n",
       "       [ 1.52512462,  2.99531953,  2.25287586,  1.91013769,  0.54994357],\n",
       "       [ 0.05420024,  0.74108211,  0.67574097,  0.36065232, -0.40073552],\n",
       "       [-0.97030149, -0.43264951, -0.35231902, -0.58320368, -1.18074271],\n",
       "       [-1.85444795, -1.34241664, -1.22673356, -1.42054648, -1.97287837]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = policy_evaluation(policy, probabilities, rewards, gamma=0.9, eps=1e-5)\n",
    "result.reshape((n_rows, n_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_values = policy_iteration(probabilities, rewards, gamma=0.9, eps=1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.97748527, 24.41942809, 21.97748528, 19.41942809, 17.47748528],\n",
       "       [19.77973674, 21.97748528, 19.77973675, 17.80176308, 16.02158677],\n",
       "       [17.80176307, 19.77973675, 17.80176308, 16.02158677, 14.41942809],\n",
       "       [16.02158676, 17.80176308, 16.02158677, 14.41942809, 12.97748528],\n",
       "       [14.41942809, 16.02158677, 14.41942809, 12.97748528, 11.67973675]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_values.reshape(n_rows, n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.97748527, 24.41942809, 21.97748528, 19.41942809, 17.47748528],\n",
       "       [19.77973674, 21.97748528, 19.77973675, 17.80176308, 16.02158677],\n",
       "       [17.80176307, 19.77973675, 17.80176308, 16.02158677, 14.41942809],\n",
       "       [16.02158676, 17.80176308, 16.02158677, 14.41942809, 12.97748528],\n",
       "       [14.41942809, 16.02158677, 14.41942809, 12.97748528, 11.67973675]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration(probabilities, rewards, gamma=0.9, eps=1e-15).reshape(n_rows, n_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_A= (0, 0)\n",
    "coord_B = (3, 3)\n",
    "grid_world_2_param = {\n",
    "    \"n_rows\": 4,\n",
    "    \"n_cols\": 4,\n",
    "    \"step_reward\": -1,\n",
    "    \"off_grid_reward\" : 0,\n",
    "    \"special_transitions\": {\n",
    "        coord_A: (coord_A, 0),\n",
    "        coord_B: (coord_B, 0),\n",
    "    }\n",
    "}\n",
    "\n",
    "grid_world_2 = GridWorld(**grid_world_2_param)\n",
    "probabilities, rewards  = grid_world_2.get_dynamics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_iteration(probabilities, rewards, gamma=1, eps=1e-15).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.ones((4*4, n_actions), dtype=np.float64)/n_actions\n",
    "policy_evaluation(policy, probabilities, rewards, gamma=1, eps=1e-40).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7730ca559ff8feee379a2381e28d3e6f0cb3410e772809d377ff1828e961366a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
